#+INCLUDE: ../orgpreamble.org

*Interpreting dummy variables and interactions* \hfill
*ARE212*: Section XX \\ \hline \bigskip

Simple example: suppose you're trying to figure out the wage effect of being married, being left-handed, and being married and left-handed. You would would want to run the following model:

$$ w_i = \alpha + \beta_1 D^1_i + \beta_2 D^2_i + \beta_3 D^1_i \times D^2_i + \varepsilon_i $$

Where $w_i$ is your wage, $D^1_i = 1$ if the person is married, and $D^2_i = 1$ if the person is left-handed. The easiest way to think about the interpretation of your mode is by considering what happens to the estimated model about when you set the dummy variables to 1 or 0. \\

For example, suppose you were interested in the average wage for unmarried ($D^1_i = 0$), left-handed people ($D^2_i = 0$). The second and fourth terms in the RHS drop from the equation, giving that $w_i = \alpha + \beta_2 + \varepsilon_i$. Since we're interested in the average and since the error term is mean zero, we can infer that the average wage for this group is $\alpha + \beta_2$. We can do the same for other groups as follows:

| Group         | $D^1_i$ | $D^2_i$ | Average wage                          |
|--------------------------+---------+---------+---------------------------------------|
| Un-married, right-handed |       0 |       0 | $\alpha$                              |
| Married, right-handed    |       1 |       0 | $\alpha + \beta_1$                    |
| Unmarried, left-handed   |       0 |       1 | $\alpha + \beta_2$                    |
| Married, left-handed     |       1 |       1 | $\alpha + \beta_1 + \beta_2 + \beta_3$ |

*Interpretation*

But what about interpreting the $\beta_1$, $\beta_2$, and $\beta_3$ coefficients themselves? Perhaps the most intuitive way to think about them is that each one is the "effect" of turning on the associated dummy variable. Accordingly, the following is (hopefully) logical.

\begin{itemize}
\item $\alpha$: The average wage for the omitted category (unmarried, right-handed)
\item $\beta_1$: The wage "effect" of being married
\item $\beta_2$: The wage "effect" of being left-handed
\item $\beta_3$: The wage "effect" of being *both* married and left-handed.
\end{itemize}

Of course, unless our data is from a randomized control trial in which we randomly assigned marriage and left-handedness to our subject population[fn:: Sounds like a fun experiment.], the coefficients we estimate aren't true effects. With a run-of-the-mill observational data set, the coefficients just represent the correlation between wage and the dummy variables. 

We're pretty much done here, but I'll leave you with this: suppose, in our observational dataset, that left-handed people are better paid and less likely to be married. Now suppose we run the following models:

$$ w_i = \alpha + \beta_1 D^1_i + \beta_2 D^2_i + \varepsilon_i $$
$$ w_i = \gamma + \delta_1 D^1_i + \mu_i $$

Which is bigger? $\beta_1$ or $\delta_1$? What if our data is from the RCT?
