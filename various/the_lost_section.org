#+AUTHOR:
#+TITLE:
#+OPTIONS:     toc:nil num:nil
#+LATEX_HEADER: \usepackage{mathrsfs}
#+LATEX_HEADER: \usepackage{graphicx}
#+LATEX_HEADER: \usepackage{hyperref}
#+LATEX_HEADER: \usepackage{booktabs}
#+LATEX_HEADER: \usepackage{dcolumn}
#+LATEX_HEADER: \usepackage{subfigure}
#+LATEX_HEADER: \usepackage[margin=1in]{geometry}
#+LATEX_HEADER: \usepackage{color}
#+LATEX_HEADER: \RequirePackage{fancyvrb}
#+LATEX_HEADER: \DefineVerbatimEnvironment{verbatim}{Verbatim}{fontsize=\small,formatcom = {\color[rgb]{0.1,0.2,0.9}}}
#+LATEX: \newcommand{\Ap}{{\bf A}^{\prime}}
#+LATEX: \newcommand{\A}{{\bf A}}
#+LATEX: \newcommand{\Bp}{{\bf B}^{\prime}}
#+LATEX: \newcommand{\B}{{\bf B}}
#+LATEX: \newcommand{\In}{{\bf I}_n}
#+LATEX: \newcommand{\In}{{\bf I}_n}
#+LATEX: \newcommand{\I}{{\bf I}}
#+LATEX: \newcommand{\Mp}{{\bf M}^{\prime}}
#+LATEX: \newcommand{\M}{{\bf M}}
#+LATEX: \newcommand{\N}{{\bf N}}
#+LATEX: \newcommand{\Q}{{\bf Q}}
#+LATEX: \newcommand{\Qp}{{\bf Q}^{\prime}}
#+LATEX: \newcommand{\W}{{\bf W}}
#+LATEX: \newcommand{\Xp}{{\bf X}^{\prime}}
#+LATEX: \newcommand{\X}{{\bf X}}
#+LATEX: \newcommand{\Y}{{\bf Y}}
#+LATEX: \newcommand{\Z}{{\bf Z}}
#+LATEX: \renewcommand{\and}{\hspace{8pt} \mbox{and} \hspace{8pt}}
#+LATEX: \newcommand{\code}[1]{\texttt{#1}}
#+LATEX: \newcommand{\email}[1]{\textcolor{blue}{\texttt{#1}}}
#+LATEX: \newcommand{\ep}{{\bf e}^\prime}
#+LATEX: \renewcommand{\b}{{\bf b}}
#+LATEX: \renewcommand{\c}{{\bf c}}
#+LATEX: \newcommand{\e}{{\bf e}}
#+LATEX: \newcommand{\f}{{\bf f}}
#+LATEX: \newcommand{\g}{{\bf g}}
#+LATEX: \newcommand{\gho}{\hat{\gamma}_1}
#+LATEX: \newcommand{\ghth}{\hat{\gamma}_3}
#+LATEX: \newcommand{\ght}{\hat{\gamma}_2}
#+LATEX: \newcommand{\id}[1]{{\bf I}_{#1}}
#+LATEX: \newcommand{\myheader}[1]{\textcolor{black}{\textbf{#1}}}
#+LATEX: \newcommand{\sigs}{\sigma^2}
#+LATEX: \newcommand{\w}{{\bf w}}
#+LATEX: \newcommand{\x}{{\bf x}}
#+LATEX: \newcommand{\yhp}{\hat{{\bf y}}^{\prime}}
#+LATEX: \newcommand{\yh}{\hat{{\bf y}}}
#+LATEX: \newcommand{\yp}{{\bf y}^{\prime}}
#+LATEX: \newcommand{\y}{{\bf y}}
#+LATEX: \newcommand{\z}{{\bf z}}
#+LATEX: \renewcommand{\P}{{\bf P}}
#+LATEX: \setlength{\parindent}{0in}
#+STARTUP: fninline

*The lost section* \hfill
*ARE212*: Section XX \\ \hline \bigskip

This is where section materials with no home come to rest.

* Projection and annihilation matrices

#+begin_src R :results output graphics :exports both :tangle yes :session
  data <- read.csv("../data/auto.csv", header=TRUE)
  names(data) <- c("price", "mpg", "weight")
  y <- matrix(data$price)
  X <- cbind(1, data$mpg, data$weight)
  n <- nrow(X)
#+end_src

#+RESULTS:

Digging deeper into the numbers, consider the projection matrix $\P = \X(\Xp\X)^{-1}\Xp$ and the residual maker matrix $\M = \In - \P$

#+begin_src R :results output graphics :exports both :tangle yes :session
n <- nrow(y)
P <- X %*% solve(t(X) %*% X) %*% t(X)
M <- diag(n) - P
#+end_src

#+RESULTS:

=R= is useful for checking the properties of these matrices, including whether $\M$ is symmetric, that is, whether $\M = \Mp$.  The function =all.equal()= does not test *exact* equality, but instead whether the supplied objects are "close enough" to be considered the same. The problem is the limits of machine precision, and rounding at the tail ends of floating point numbers.

#+begin_src R :results output graphics :exports both :tangle yes :session
all.equal(M, t(M))
#+end_src

#+RESULTS:
: [1] TRUE

If we want to test for exact equality, we set the tolerance to zero, and the function will return a message with the mean relative difference between elements --- which is clearly very close to zero.

#+begin_src R :results output graphics :exports both :tangle yes :session
all.equal(M, t(M), tol=0)
#+end_src

#+RESULTS:
: [1] "Mean relative difference: 5.547715e-15"

The residual maker matrix should also be idempotent, or $\M = \M\M$.

#+begin_src R :results output graphics :exports both :tangle yes :session
all.equal(M, M %*% M)
#+end_src

#+RESULTS:
: [1] TRUE

We could also verify that $\M = \Mp \M = \Mp \Mp = \M \M \M \M$, and so on. Instead, let's move on to the sums of squares.

* Calculating $R^2_{uc}$

Finally, we can use $\M$ and $\P$ to examine the different components of the variation in the dependent variable as they relate to the OLS estimate. We'll then use this to calculate the $R_^2{uc}$. First, recall from lecture that:
\begin{equation}
\label{eq:ss}
\yp\y = \yhp\yh + \ep\e
\end{equation}
First, define the relevant variables:

#+begin_src R :results output graphics :exports both :tangle yes :session
e <- M %*% y
y.hat <- P %*% y
epe <- t(e) %*% e
yhpyh <- t(y.hat) %*% y.hat
ypy <- t(y) %*% y
#+end_src

#+RESULTS:

Then check the condition in Eq. (\ref{eq:ss}):

#+begin_src R :results output graphics :exports both :tangle yes :session
all.equal(ypy, yhpyh + epe)
#+end_src

#+RESULTS:
: [1] TRUE

This is neat, but what

* Sum of squared residuals

Suppose that $\b$ is the $2 \times 1$ least squared coefficient vector
in the regression of $\y$ on $\X_2$.  Suppose that $\c$ is some other
$2 \times 1$ vector.  We are asked to show that
\begin{equation}
(\y - \X\c)^{\prime} (\y - \X\c) - (\y - \X\b)^{\prime} (\y - \X\b) = (\c - \b)^{\prime}
\Xp \X (\c - \b)
\label{eq:one}
\end{equation}
 We could prove this with matrix algebra. In fact,
we are asked to prove this fact with matrix algebra in the problem
set.  But matrix algebra is for chumps --- or very smart and kind
people.  Let's check the equality using =R= by choosing any arbitrary
$\c$.

#+begin_src R :results output graphics :exports both :tangle yes :session
  b <- solve(t(X2) %*% X2) %*% t(X2) %*% y
  c <- c(-3, 5)
#+end_src

#+RESULTS:

For simplicity of notation, define $\M$ and $\N$ to be the following:

#+begin_src R :results output graphics :exports both :tangle yes :session
  M <- y - X2 %*% b
  N <- y - X2 %*% c
#+end_src

#+RESULTS:

Now, we can check both sides of the equality:

#+begin_src R :results output graphics :exports both :tangle yes :session
  lhs <- floor(t(N) %*% N - t(M) %*% M)
  rhs <- floor(t(c - b) %*% t(X2) %*% X2 %*% (c - b))
  all(lhs == rhs)
#+end_src

#+RESULTS:
: [1] TRUE

Note, however, that the order of $\c$ and $\b$ doesn't matter:

#+begin_src R :results output graphics :exports both :tangle yes :session
  rhs.alt <- floor(t(b - c) %*% t(X2) %*% X2 %*% (b - c))
  all(lhs == rhs.alt)
#+end_src

#+RESULTS:
: [1] TRUE

This result is because we are effectively looking at the sum of the
squared difference between the two vectors.  The ordering in the
difference calculation doesn't matter if it is subsequently squared.
Consider the property $\V(\c\X) = \c\V(\X)\c^{\prime}$ for a vector
$\c$ or $\V(a\X) = a^2\V(\X)$ for a scalar $a$.  The right-hand side
of Equation (\ref{eq:eq}) is effectively a nested, squared matrix,
which has to yield positive entries (and a postive scalar if the
result is $1 \times 1$):

#+begin_src R :results output graphics :exports both :tangle yes :session
G <- X2 %*% (b - c)
t(G) %*% G
#+end_src

#+RESULTS:
:            [,1]
: [1,] 6209641706


