#+INCLUDE: ../orgpreamble.org

*=ggplot2= and Maximum Likelihood* \hfill
*ARE212*: Section 06 \\ \\

This section, we'll take another crack at =ggplot2= before spending the remainder of the time on Maximum Likelihood estimation.

* The grammar of =ggplot2=
Last week we demonstrated the use of =ggplot2= as an alternative to the =R= base graphics without really talking about how =ggplot2= works or why it might be a better graphing option. Today we'll attempt to fill that gap[fn:: Sources for this section: http://vita.had.co.nz/papers/layered-grammar.pdf and http://www.slideshare.net/AndrewZieffler/unit-02ggplot2.]. \\

=ggplot2= is an implementation of the "grammar of graphics," described in a book of the same title by Leland Wilkensen. The idea is that graphical representations of data, like language, have a logical grammatical structure. Most graphing packages ignore this structure and create one-off solutions for every different kind of graph that we might want to display. This is inefficient, and therefore displeasing to economists. =ggplot2= allows users to control the composition of statistical graphs by directly controlling the grammar of the graphical components. \\

Plots in =ggplot2= are built by putting together separate component parts. The three crucial components that we'll think about for now are:
- data
- aesthetics
- layers/geometric shapes

There are more, but these are the important ones. We'll tackle each separately. \\

*Data* \\
The /data/ for =ggplot2= should always be packaged into a data frame. After loading the =ggplot2= library, we'll load the =R= iris dataset to demonstrate:

#+begin_src R :results none :exports code :tangle yes :session
library(ggplot2)
data(iris)
ggplot(data = iris, ... ) # not a real command
#+end_src

The first argument we pass to =ggplot()= will be the data frame that we intend represent graphically. \\

*Aesthetics*\\
The second required argument for =ggplot()= is the /aesthetic mapping/ of the plot. You might think of aesthetics here as "things that you can see", such as the position of the data on the axes, the color, the shape, et cetera. We use this argument to map the data we have into the aesthetics that we're going to display. Now we can create and display the =ggplot2= object =g= using our data an aesthetics.

#+begin_src R :results output :exports code :tangle yes :session
(g <- ggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width)))
#+end_src

#+RESULTS:
: Error: No layers in plot

*Layers/geometric shapes*
Or not. Why are we getting an error? We've specified our data, and our aesthetics, but not our graphical layers (i.e. geometric shapes). Here, we'll add a layer of points:

#+begin_src R :results output graphics :file inserts/fig3.png :width 600 :height 200 :session :tangle yes :exports bot
g + geom_point()
#+end_src

#+RESULTS:
[[file:inserts/fig3.png]]

We can also specify additional aesthetic options for each layer. Below, we'll tell =ggplot2= to graph the points again, this time specifying that each species should have a different color. Aesthetic options specified in the =ggplot()= function are the default for all layers, but aesthetics specified within layers can override the defaults for that layer only.

#+begin_src R :results output graphics :file inserts/fig5.png :width 600 :height 200 :session :tangle yes :exports bot
g + geom_point(aes(color=Species))
#+end_src

#+RESULTS:
[[file:inserts/fig5.png]]

We're not limited to scatterplots. Although it's a little nonsensical, we can add boxplots of =Sepal.Width= to the current graph, after sorting the groups into their (rounded) =Sepal.Length= categories.

#+begin_src R :results output graphics :file inserts/fig6.png :width 600 :height 200 :session :tangle yes :exports bot
g + geom_point(aes(color=Species)) + geom_boxplot(aes(group = round(Sepal.Length)), outlier.size = 0)
#+end_src

#+RESULTS:
[[file:inserts/fig6.png]]

Initially, putting together the grammar of =ggplot2= may seem cumbersome. In fact, the code to construct simple scatterplots or histograms in =ggplot2= is almost certainly going to be more complex than a simple =plot()= or =hist()= from the base graphics package[fn:: In fact, =ggplot2= provides a function called =qplot()= that replicates the simpler syntax from the base graphing package, if you prefer.]. But as your graphics needs become more complex, you will almost certainly find that =ggplot2= scales much better and is far more powerful than the base functions provided by =R=.

* Maximum likelihood

In this section, we're going to step away from the world of OLS to examine the maximum likelihood (ML) estimator. Max has already covered the theory behind ML in class, so we'll be exploring the theory with some empirical examples that demonstrate how to perform ML using =R=. \\

Before that, though, let's begin with some intuion. The general principle behind ML estimation is that we have observe some data vector $\z$ which we assume to be drawn from a probability distribution $f(\z; \th)$, where $\theta$ is a vector of parameters that characterize the distribution. Once we pick the distribution, we then use either analytical or computational to determine the $\thh$ that best explains the data $\z$ that we observe. In other words, in order to find $\thh$, we simply search for the parameters that maximize the probability of observing the values of our data $\z$. \\

If you're like me, you found the above explanation mostly baffling the first few times you heard it. Rather than repeating it to you once more, let's dive into the estimation process with some specific examples. \\

*ML on a Bernouilli distribution*\\

The outcome of flipping a (potentially rigged) coin is described by the *Bernouilli distribution*, which is a special case of the binomial distribution. A Bernouilli random variable is 1 with probability $p$ and 0 with probability $1-p$. $p$ is the sole parameter to characterize this distribution, so in this case we have that $\th \equiv p$. \\

Suppose that we observe a sequence of Bernouilli draws, each with the same unknown $p$. From the sequence, which looks something like $\x =$ [0 1 0 0 0 0 1 1 0 1 1 ...], we want to estimate $p$ using $\hat{p}$. We can do this using ML. \\

As we have done before, we'll use a simulation method so that we can set the "true" data to test our method[fn:: This section is based on a similar tutorial by John Myles White, available at: http://www.johnmyleswhite.com/notebook/2010/04/21/doing-maximum-likelihood-estimation-by-hand-in-r/.]. First, we set up preliminaries and create our (very long) sequence of flips:

#+begin_src R :results output graphics :exports both :tangle yes :session
set.seed(42)
flips <- 1000
p.true <- 0.59
x <- rbinom(flips, 1, p.true)
head(x)
#+end_src

#+RESULTS:
: [1] 0 0 1 0 0 1

Next, we'll use probability theory to define our likelihood function, which is just the joint probability of observing the particular sequence of $\x$ that we see, given that each flip is identically and independently distributed Bernoulli with parameter given some $\tp$. Remember, our goal is to choose $\tp$ to maximize the likelihood function.

$$ \text{L}(\tp}) = \prod_{i=1}^{1000} \tp^{x_i} (1-\tp)^{1-x_i} $$

We can take logs of the likelihood function to produce the often-more-tractable log-likelihood function:

$$ \like(\tp) = \sum_{i=1}^{1000} x_i \ln(\tp) + (1-x_i) \ln(1-\tp) $$

By taking the derivative of the log likelihood function we can show[fn:: But won't. See http://mathworld.wolfram.com/MaximumLikelihood.html.] analytically that the best estimate of $p$ is the sample mean of $\x$. This gives us a benchmark against which we can compare the estimates we get from optimization.

#+begin_src R :results output graphics :exports both :tangle yes :session
(analytical.est <- mean(x))
#+end_src

#+RESULTS:
: [1] 0.619

We can actually optimize over either the likelihood or the log-likelihood function. Let's create both of them as functions that take in =p=, our guess at the value for $p$, and =x=, our sequences of 0s and 1s. The output of both functions will be the likelihood (or log-likelihood) of observing =x= given our guess at =p=.

#+begin_src R :results output graphics :exports both :tangle yes :session
likelihood <- function(p, x) {
  prod(p^x * (1-p)^(1-x))
}

log.likelihood <- function(p, x) {
  sum(x * log(p) + (1-x) * log(1-p))
}
#+end_src

#+RESULTS:

Now, let's use =sapply()= to calculate the values of the these functions over the range $[0,1]$, since we know that the true value of $p$ has to be within that range.
#+begin_src R :results output graphics :exports both :tangle yes :session
poss.p <- seq(0,1,0.001)
like <- sapply(poss.p, likelihood, x = x)
loglike <- sapply(poss.p, log.likelihood, x = x)
#+end_src
#+RESULTS:

So what is this thing we're trying to maximize, exactly? Let's find out. Using =ggplot2=, we plot the likelihood function:

#+begin_src R :results output graphics :file inserts/fig1.png :width 600 :height 300 :session :tangle yes :exports bot
data <- data.frame(poss.p,like,loglike)
library(ggplot2)
ggplot(data=data, aes(x=poss.p, y=like))  + geom_line()
#+end_src

#+RESULTS:
[[file:inserts/fig1.png]]

As you can see, the, function seems to peak sharply around $p=0.60$. That's pretty good! Since log is a monotone operator[fn:: I.E. $x > y \leftrightarrow \ln(x) > \ln(y)$], we should expect to see the maximum at the same point $p$:

#+begin_src R :results output graphics :file inserts/fig2.png :width 600 :height 300 :session :tangle yes :exports bot
ggplot(data=data, aes(x=poss.p, y=loglike)) + geom_line()
#+end_src

#+RESULTS:
[[file:inserts/fig2.png]]


And we do! More or less. However, if we want to be more precise about our estimate, we ought to use an optimization algorithm. Let's try it, first with the likelihood function.

#+begin_src R :results output graphics :exports both :tangle yes :session
opt.like <- optimize(f = likelihood, c(0,1), maximum = T, x = x)
cbind(opt.like$maximum, opt.like$objective)
#+end_src

#+RESULTS:
:           [,1]          [,2]
: [1,] 0.6189997 2.448654e-289

That's pretty darn close to our analytical estimate, the sample mean, which was =0.619=. =optimize()= is a very straightforward function: with this call, we pass it the function we want to optimize, our limits on the parameter of interest $p$, a flag that indicates we want it to maximize the likelihood function, and an additional parameter that we pass through to the likelihood function. We can use it again to find the estimate of $p$ given by the log-likelihood function:

#+begin_src R :results output graphics :exports both :tangle yes :session
opt.loglike <- optimize(f = log.likelihood, c(0,1), maximum = T, x = x)
cbind(opt.loglike$maximum, opt.loglike$objective)
#+end_src

#+RESULTS:
:           [,1]      [,2]
: [1,] 0.6190052 -664.5516

Cool. You'll recall from lecture that we use the log-likelihood because it provides analytical convenience --- it's typically much simpler to take the derivative over a sum than a product. But you might not know that the log-likelihood has computational advantages as well! To see this, try setting =flips = 100000=. Cover your ears. \\

*Linear regression and maximum likelihood*

Now that we've got a little maximum likelihood experience under our belts, let's see if we can replicate the theoretical results from lecture by fitting a linear regression model using ML rather than OLS. To use ML, we need to make some sort of distributional assumption that we can use to estimate our parameters. Following the lecture notes (Section 2.7.3), we use the convenient assumption A6, which gives us that $\bf{\varepsilon} | \X \sim N(0,\sigma^2 \I_n)$. From the linearity of our model, we get that $\y | \X \sim(\X \beta, \sigma^2 \I_n)$, which we plug into the pdf for a multivariate normal distribution. Taking logs over the distribution gives us:

$$ \log L(\tilde{\b}, \tilde{\sigma}^2) = -\frac{n}{2} \log(2\pi) - \frac{n}{2} \log(\tilde{\sigma}^2) - \frac{1}{2\tilde{\sigma}^2} (\y - \X \b)'(\y - \X \b) $$

Following Max's notes and solve for $\b$ and $\sigma^2$ analytically, we should find that $\b_{ML} = \b_{OLS} = (X'X)^{-1} X'y$ and that $\sigma^2 = \frac{\e'\e}{n}$. *Or*, we could think jointly estimate $\b$ and $\sigma^2$ using the =optim= command (which is similar to =optimize=, except that it is capable of optimizing over more than one dimension at once).

#+begin_src R :results output graphics :exports both :tangle yes :session
n <- 2000
set.seed(42)
X <- cbind(1,runif(n))
eps <- rnorm(n)
b.true <- rbind(5,3)
y <- X %*% b.true + eps
#+end_src

#+RESULTS:

#+begin_src R :results output graphics :exports both :tangle yes :session
log.likelihood.optim <- function(theta) {
  sigma2 <- tail(theta, n = 1)
  b <- theta[1:nrow(b)]
  e <- y - X %*% b
  output <- -n/2*log(2*pi) - n/2*log(sigma2) - 1/(2 * sigma2) * t(e) %*% e
  return(-output)
}
optim(par = c(3,2,1), fn = log.likelihood.optim)$par
#+end_src

#+RESULTS:
: [1] 4.988389 3.014805 1.015007

The first argument to =optim= is a set of starting parameters for $\th$. In general, in order to maximize our chances of finding the correct optimum (and to increase the rate of convergence), we want to choose reasonable parameters. The second argument is just the log-likelihood function that we created. Let's confirm that $OLS$ gives the same parameter estimates for $\beta$ as ML.

#+begin_src R :results output graphics :exports both :tangle yes :session
OLS <- function(y,X) { b <- solve(t(X) %*% X) %*% t(X) %*% y }
(b <- OLS(y,X))
#+end_src

#+RESULTS:
:          [,1]
: [1,] 4.988142
: [2,] 3.015419

Whew! They're nearly exactly the same! We can also calculate the $s^2_{OLS}$ and the $\tilde{\sigma}^2_{ML}$, which we know to be different.

#+begin_src R :results output graphics :exports both :tangle yes :session
e <- y - X %*% b
k <- ncol(X)
cbind(s2.OLS <- (t(e) %*% e) / (n - k), s2.ML <- (t(e) %*% e) / n)
#+end_src

#+RESULTS:
:          [,1]     [,2]
: [1,] 1.016147 1.015131

* Another ML example - when intractable analytically		   :noexport:

For our last ML example, we'll look at a situation where no analytic solution exists. \\

Suppose that I tell you that $\X$ is a random vector, where each
$\X_i$ is generated from a common density function $\theta / (\theta +
\X_i)^2$.  We want to estimate $\theta$ by maximum likelihood, given
the data set =mle.txt= from Freedman (2009).  It can be shown that the
log-likelihood function is given by the following, where $n$ is the
number of observations: $$\like = n \log \theta - 2 \sn{i}
\log(\theta + \X_i)$$ First, we will plot the log-likelihood function
as a function of $\theta$, and then find the maximum with =optimize=.

#+begin_src R :results output graphics :exports both :tangle yes :session
  data <- read.csv("mle.txt", header = FALSE)
  data <- read.csv("http://dl.dropbox.com/u/5365589/mle.txt", header = FALSE)

  logLik <- function(theta, X = data) {
    n <<- nrow(X)
    n * log(theta) - 2 * sum(log(theta + X))
  }
#+end_src

#+RESULTS:

To maximize this function with respect to $\theta$, we don't have to
do any math.  And in fact, for this function, there is no explicit
function for the maximum likelihood estimate, and we have to find the
estimate through numerical optimization.

#+begin_src R :results output graphics :exports both :tangle yes :session
  optimize(logLik, interval=c(-100, 100), maximum=TRUE)
  suppressWarnings(opt <- optimize(logLik, interval=c(-100, 100), maximum=TRUE))
  (theta.hat <- opt$maximum)
#+end_src

#+RESULTS:
#+begin_example
$maximum
[1] 22.50975

$objective
[1] -249.3968

Warning messages:
1: In log(theta) : NaNs produced
2: In lapply(X = x, FUN = .Generic, ...) : NaNs produced
3: In optimize(logLik, interval = c(-100, 100), maximum = TRUE) :
  NA/Inf replaced by maximum positive value
[1] 22.50975
#+end_example

We can compute the asymptotic variance in a variety of ways, but
perhaps the most direct is $[- \likepp]^{-1}$:

#+begin_src R :results output graphics :exports both :tangle yes :session
  dd.logLik <- function(theta, X = data) {
    -1 * (n / theta^2) + 2 * sum(1 / (theta + X)^2)
  }

  (asy.var <- -1 / dd.logLik(theta.hat))
#+end_src

#+RESULTS:
: [1] 30.12321
