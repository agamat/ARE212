#+AUTHOR:
#+TITLE:
#+OPTIONS:     toc:nil num:nil
#+LATEX_HEADER: \usepackage{mathrsfs}
#+LATEX_HEADER: \usepackage{graphicx}
#+LATEX_HEADER: \usepackage{hyperref}
#+LATEX_HEADER: \usepackage{booktabs}
#+LATEX_HEADER: \usepackage{dcolumn}
#+LATEX_HEADER: \usepackage{subfigure}
#+LATEX_HEADER: \usepackage[margin=1in]{geometry}
#+LATEX_HEADER: \usepackage{color}
#+LATEX_HEADER: \RequirePackage{fancyvrb}
#+LATEX_HEADER: \DefineVerbatimEnvironment{verbatim}{Verbatim}{fontsize=\small,formatcom = {\color[rgb]{0.1,0.2,0.9}}}
#+LATEX: \newcommand{\Ap}{{\bf A}^{\prime}}
#+LATEX: \newcommand{\A}{{\bf A}}
#+LATEX: \newcommand{\Bp}{{\bf B}^{\prime}}
#+LATEX: \newcommand{\B}{{\bf B}}
#+LATEX: \newcommand{\In}{{\bf I}_n}
#+LATEX: \newcommand{\In}{{\bf I}_n}
#+LATEX: \newcommand{\I}{{\bf I}}
#+LATEX: \newcommand{\Mp}{{\bf M}^{\prime}}
#+LATEX: \newcommand{\M}{{\bf M}}
#+LATEX: \newcommand{\N}{{\bf N}}
#+LATEX: \newcommand{\Q}{{\bf Q}}
#+LATEX: \newcommand{\Qp}{{\bf Q}^{\prime}}
#+LATEX: \newcommand{\W}{{\bf W}}
#+LATEX: \newcommand{\Xp}{{\bf X}^{\prime}}
#+LATEX: \newcommand{\X}{{\bf X}}
#+LATEX: \newcommand{\Y}{{\bf Y}}
#+LATEX: \newcommand{\Z}{{\bf Z}}
#+LATEX: \renewcommand{\and}{\hspace{8pt} \mbox{and} \hspace{8pt}}
#+LATEX: \newcommand{\code}[1]{\texttt{#1}}
#+LATEX: \newcommand{\email}[1]{\textcolor{blue}{\texttt{#1}}}
#+LATEX: \newcommand{\ep}{{\bf e}^\prime}
#+LATEX: \newcommand{\e}{{\bf e}}
#+LATEX: \newcommand{\f}{{\bf f}}
#+LATEX: \newcommand{\g}{{\bf g}}
#+LATEX: \newcommand{\gho}{\hat{\gamma}_1}
#+LATEX: \newcommand{\ghth}{\hat{\gamma}_3}
#+LATEX: \newcommand{\ght}{\hat{\gamma}_2}
#+LATEX: \newcommand{\id}[1]{{\bf I}_{#1}}
#+LATEX: \newcommand{\myheader}[1]{\textcolor{black}{\textbf{#1}}}
#+LATEX: \newcommand{\sigs}{\sigma^2}
#+LATEX: \newcommand{\w}{{\bf w}}
#+LATEX: \newcommand{\x}{{\bf x}}
#+LATEX: \newcommand{\yhp}{\hat{{\bf y}}^{\prime}}
#+LATEX: \newcommand{\yh}{\hat{{\bf y}}}
#+LATEX: \newcommand{\yp}{{\bf y}^{\prime}}
#+LATEX: \newcommand{\y}{{\bf y}}
#+LATEX: \newcommand{\z}{{\bf z}}
#+LATEX: \renewcommand{\P}{{\bf P}}
#+LATEX: \setlength{\parindent}{0in}
#+STARTUP: fninline

*Ordinary Least Squares* \hfill
*ARE212*: Section 03 \\ \hline \bigskip

This section continues our work with =auto.csv= from the first section, using the matrix operations we learned in the second. \\

The use of canned routines is not permitted for most of this class; you'll have to write the econometric routines from first principles. First, create matrices of the data, since we will be working mainly with matrix operations.  Let $\y$ be the dependent variable, price, and let $\X$ be a matrix of the other car characteristics, along with a column of ones prepended.  The =cbind()= function binds the columns horizontally and coerces the =matrix= class.

#+BEGIN_SRC R :results output :exports none :session :tangle yes
require(foreign)
data <- read.csv("auto.csv", header=TRUE)
names(data) <- c("price", "mpg", "weight")
#+END_SRC

#+RESULTS:

#+begin_src R :results output graphics :exports both :tangle yes :session
y <- matrix(data$price)
X <- cbind(1, data$mpg, data$weight)
head(X)
#+end_src

#+RESULTS:
:      [,1] [,2] [,3]
: [1,]    1   22 2930
: [2,]    1   17 3350
: [3,]    1   22 2640
: [4,]    1   20 3250
: [5,]    1   15 4080
: [6,]    1   18 3670

Note that =R= is doing something clever here. We only passed it a single =1= variable, and yet it has created an entire column vector full of ones! This is because =cbind()= automatically ensures that every column is as long as the longest column in the matrix, recycling data as needed. \\

Just to make sure that our matrices will be conformable when we regress =y= on =X=, check that the number of observations are the same in both variables.

#+begin_src R :results output graphics :exports both :tangle yes :session
dim(X)[1] == nrow(y)
#+end_src

#+RESULTS:
: [1] TRUE

Using the matrix operations described in the previous section, we can quickly estimate the ordinary least squared parameter vector.

#+begin_src R :results output graphics :exports both :tangle yes :session
beta <- solve(t(X) %*% X) %*% t(X) %*% y
beta
#+end_src

#+RESULTS:
:             [,1]
: [1,] 1946.068668
: [2,]  -49.512221
: [3,]    1.746559

That's it! And although you're not allowed to use it in your problem sets, =lm()= is a nice tool for checking our results.

#+begin_src R :results output graphics :exports both :tangle yes :session
lm(y ~ X - 1)
#+end_src

#+RESULTS:
:
: Call:
: lm(formula = y ~ X - 1)
:
: Coefficients:
:       X1        X2        X3
: 1946.069   -49.512     1.747

They match! Thank goodness. If you're interested in knowing what =lm()= does, I highly recommend reading through relevant =R= documentation[fn:: Remember, you can do this using =?lm=.]. \\

Digging deeper into the numbers, consider the projection matrix $\P = \X(\Xp\X)^{-1}\Xp$ and the residual maker matrix $\M = \In - \P$

#+begin_src R :results output graphics :exports both :tangle yes :session
n <- nrow(y)
P <- X %*% solve(t(X) %*% X) %*% t(X)
M <- diag(n) - P
#+end_src

#+RESULTS:

=R= is useful for checking the properties of these matrices, including whether $\M$ is symmetric, that is, whether $\M = \Mp$.  The function =all.equal()= does not test *exact* equality, but instead whether the supplied objects are "close enough" to be considered the same. The problem is the limits of machine precision, and rounding at the tail ends of floating point numbers.

#+begin_src R :results output graphics :exports both :tangle yes :session
all.equal(M, t(M))
#+end_src

#+RESULTS:
: [1] TRUE

If we want to test for exact equality, we set the tolerance to zero, and the function will return a message with the mean relative difference between elements --- which is clearly very close to zero.

#+begin_src R :results output graphics :exports both :tangle yes :session
all.equal(M, t(M), tol=0)
#+end_src

#+RESULTS:
: [1] "Mean relative difference: 5.547715e-15"

The residual maker matrix should also be idempotent, or $\M = \M\Mp$.

#+begin_src R :results output graphics :exports both :tangle yes :session
all.equal(M, M %*% t(M))
#+end_src

#+RESULTS:
: [1] TRUE

Finally, we can examine the different components of the variation in the dependent variable, as they relate to the OLS estimate. Specifically, we can show that the total sum of square is equal to the sum of the residual and estimated sum of squares:
\begin{equation}
\label{eq:ss}
\yp\y = \yhp\yh + \ep\e
\end{equation}
First, define the relevant variables:

#+begin_src R :results output graphics :exports both :tangle yes :session
e <- M %*% y
y.hat <- P %*% y
rss <- t(e) %*% e
ess <- t(y.hat) %*% y.hat
tss <- t(y) %*% y
#+end_src

#+RESULTS:

Then check the condition in Eq. (\ref{eq:ss}):

#+begin_src R :results output graphics :exports both :tangle yes :session
all.equal(tss, ess + rss)
#+end_src

#+RESULTS:
: [1] TRUE

Cool.

* Additional puzzles

1. Write a function =wt.coef()= that will return the OLS coefficient on weight from the regression of car price on the covariate matrix described above.

2. Adjust the function to return a list of coefficients from the same linear regression, appropriately named.

3. Find the estimate of the covariance matrix $\sigs (\Xp\X)^{-1}$ and show that the residuals and covariate matrix are orthogonal.
