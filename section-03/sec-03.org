#+AUTHOR:      Dan Hammer
#+TITLE:       ARE212: Section 03
#+OPTIONS:     toc:nil num:nil 
#+LATEX_HEADER: \usepackage{mathrsfs}
#+LATEX_HEADER: \usepackage{graphicx}
#+LATEX_HEADER: \usepackage{subfigure}
#+LATEX: \newcommand{\Rsq}{R^{2}}
#+LATEX: \newcommand{\ep}{{\bf e}^\prime}
#+LATEX: \renewcommand{\e}{{\bf e}}
#+LATEX: \renewcommand{\b}{{\bf b}}
#+LATEX: \renewcommand{\bp}{{\bf b}^{\prime}}
#+LATEX: \renewcommand{\bs}{{\bf b}^{*}}
#+LATEX: \renewcommand{\I}{{\bf I}}
#+LATEX: \renewcommand{\X}{{\bf X}}
#+LATEX: \renewcommand{\M}{{\bf M}}
#+LATEX: \renewcommand{\A}{{\bf A}}
#+LATEX: \renewcommand{\B}{{\bf B}}
#+LATEX: \renewcommand{\C}{{\bf C}}
#+LATEX: \renewcommand{\P}{{\bf P}}
#+LATEX: \renewcommand{\Xp}{{\bf X}^{\prime}}
#+LATEX: \renewcommand{\Xsp}{{\bf X}^{*\prime}}
#+LATEX: \renewcommand{\Xs}{{\bf X}^{*}}
#+LATEX: \renewcommand{\Mp}{{\bf M}^{\prime}}
#+LATEX: \renewcommand{\y}{{\bf y}}
#+LATEX: \renewcommand{\ys}{{\bf y}^{*}}
#+LATEX: \renewcommand{\yp}{{\bf y}^{\prime}}
#+LATEX: \renewcommand{\ysp}{{\bf y}^{*\prime}}
#+LATEX: \renewcommand{\yh}{\hat{{\bf y}}}
#+LATEX: \renewcommand{\yhp}{\hat{{\bf y}}^{\prime}}
#+LATEX: \renewcommand{\In}{{\bf I}_n}
#+LATEX: \setlength{\parindent}{0in}
#+STARTUP: fninline

The idea behind this section is to study the behavior of the centered
and uncentered $\Rsq$ as cofactors are incrementally included in the
regression.  First, we must create a random matrix, where each
variable is drawn from a standard uniform distribution.  All elements
are independent and identically distributed, so we can create a very
long, random vector and reshape it into a rectangular matrix.  For
convenience and practice writing functions in =R=, we show a general
function that accepts the dimensions ($n$ rows and $k$ columns) of the
matrix.  The function generates a long vector of length $n \cdot k$
and then reshapes it into an $n \times k$ matrix.

#+begin_src r :results output graphics :exports both :tangle yes :session
  random.mat <- function(n, k) {
    v <- runif(n*k)
    matrix(v, nrow=n, ncol=k)
  }
#+end_src

The function bound to =random.mat()= behaves as we would expect:

#+begin_src r :results output graphics :exports both :tangle yes :session
  random.mat(3,2)
#+end_src

#+RESULTS:
:            [,1]      [,2]
: [1,] 0.54596729 0.1595011
: [2,] 0.09682105 0.8549968
: [3,] 0.92691069 0.5953242

Another useful function for this section will be to create a square
demeaning matrix $\A$ of dimension $n$.  The following function just
wraps a few algebraic maneuvers, so that subsequent code is easier to
read.

#+begin_src r :results output graphics :exports both :tangle yes :session
  demean.mat <- function(n) {
    ones <- rep(1, n)
    diag(n) - (1/n) * ones %*% t(ones)
  }
#+end_src

As is described in the notes, pre-multiplying a matrix $\B$ by $\A$
will result in a matrix $\C = \A\B$ of deviations from the column
means of $\B$. Check that this is true.

#+begin_src r :results output graphics :exports both :tangle yes :session
  A <- demean.mat(3)
  B <- matrix(1:9, nrow=3)
  col.means <- apply(B, 2, mean)
  C <- apply(B, 1, function(x) {x - col.means})
  all.equal(A %*% B, t(C))
#+end_src

#+RESULTS:
: [1] TRUE

Alright, we're ready to apply the functions to real data in order to
calculate the centered $\Rsq$. First, read in the data to conform to
equation (2.37) on page 14 of the lecture notes, and identify the
number of observations $n$ for later use:
#+begin_src r :results output graphics :exports both :tangle yes :session
  data <- read.csv("../data/auto.csv", header=TRUE)
  names(data) <- c("price", "mpg", "weight")
  y <- matrix(data$price)
  X2 <- cbind(data$mpg, data$weight)
  n <- nrow(X2)
#+end_src

#+RESULTS:

The centered $\Rsq$ is defined according to equation (2.41) as
follows:
\begin{equation}
\label{eq:rsq}
\Rsq = \frac{\bp_{2}\Xsp_{2}\Xs_{2}\b_{2}}{\ysp\ys},
\end{equation} where $\ys = \A\y$, $\Xs_2$ = \A\X_2$, and $\b_2 =
(\Xsp_{2}\Xs_{2})^{-1}\Xsp_{2}\ys$.  Noting that $\A$ is both
symmetric and idempotent, we can rewrite Eq. (\ref{eq:rsq}) in terms
of matrices already defined, thereby simplifying the subsequent code
dramatically.  From my limited experience with programming, the best
code is that which reflects the core idea of the procedure; more time
spent with a pen and paper and not in =R= will almost always yield
more readable code, and more readable code yields fewer errors and
suggests quick extensions.  That said, note that $\Xsp_{2}\Xs_{2} =
\Xp_2\Ap\A\X_2 = \Xp_2\A\A\X_2 = \Xp_2\A\X_2$ and similarly that
$\ysp\ys = \yp\A\y$ and $\Xsp_{2}\ys = \Xp_{2}\A\y$. If we write a
more general function, though, we can apply it to an arbitrary
dependent vector and associated cofactor matrix:
#+begin_src r :results output graphics :exports both :tangle yes :session
  R.squared <- function(y, X) {
    n <- nrow(X)
    A <- demean.mat(n)
    xtax <- t(X) %*% A %*% X
    ytay <- t(y) %*% A %*% y
    b2 <- solve(xtax) %*% t(X) %*% A %*% y
    t(b2) %*% xtax %*% b2 / ytay
  }
  
  R.squared(y, X2)
#+end_src

#+RESULTS:
:           [,1]
: [1,] 0.2933891

Without some penalty for addtional cofactors, the $\Rsq$ will
monotonically increase with the number of columns in the cofactor
matrix $\X$.  We can plot this function, mostly as an introduction to
very simple plots in =R=. 

#+begin_src r :results output graphics :exports both :file inserts/graph1.png :tangle yes :session
  n <- nrow(X2)
  X.rnd <- random.mat(n, 70)
  res <- rep(0, 70)
  for(i in 1:70) {
    X.ext <- cbind(X2, X.rnd[, 1:i])
    res[i] <- R.squared(y, X.ext)
  }
  plot(res)
#+end_src

#+RESULTS:
[[file:inserts/graph1.png]]

It may be difficult to get a sense of the shape of the curve based on
a single draw for the random matrix.  We can calculate the
relationship between $\Rsq$ and the number of cofactors --- or we can
bootstrap an estimate for each index, which we will do in a subsequent
section to illustrate bootstrapping in =R=.
