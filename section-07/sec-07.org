#+AUTHOR:     
#+TITLE:      
#+OPTIONS:     toc:nil num:nil 
#+LATEX_HEADER: \usepackage{mathrsfs}
#+LATEX_HEADER: \usepackage{graphicx}
#+LATEX_HEADER: \usepackage{booktabs}
#+LATEX_HEADER: \usepackage{dcolumn}
#+LATEX_HEADER: \usepackage{subfigure}
#+LATEX_HEADER: \usepackage[margin=1in]{geometry}
#+LATEX_HEADER: \RequirePackage{fancyvrb}
#+LATEX_HEADER: \DefineVerbatimEnvironment{verbatim}{Verbatim}{fontsize=\small,formatcom = {\color[rgb]{0.1,0.2,0.9}}}
#+LATEX: \newcommand{\Rb}{{\bf R}}
#+LATEX: \newcommand{\Rbp}{{\bf R}^{\prime}}
#+LATEX: \newcommand{\Rsq}{R^{2}}
#+LATEX: \newcommand{\ep}{{\bf e}^\prime}
#+LATEX: \renewcommand{\e}{{\bf e}}
#+LATEX: \renewcommand{\bh}{\hat{\beta}}
#+LATEX: \renewcommand{\ah}{\hat{\alpha}}
#+LATEX: \renewcommand{\r}{{\bf r}}
#+LATEX: \renewcommand{\bp}{{\bf b}^{\prime}}
#+LATEX: \renewcommand{\bs}{{\bf b}^{*}}
#+LATEX: \renewcommand{\I}{{\bf I}}
#+LATEX: \renewcommand{\X}{{\bf X}}
#+LATEX: \renewcommand{\M}{{\bf M}}
#+LATEX: \renewcommand{\A}{{\bf A}}
#+LATEX: \renewcommand{\B}{{\bf B}}
#+LATEX: \renewcommand{\C}{{\bf C}}
#+LATEX: \renewcommand{\P}{{\bf P}}
#+LATEX: \renewcommand{\Xp}{{\bf X}^{\prime}}
#+LATEX: \renewcommand{\Xsp}{{\bf X}^{*\prime}}
#+LATEX: \renewcommand{\Xs}{{\bf X}^{*}}
#+LATEX: \renewcommand{\Mp}{{\bf M}^{\prime}}
#+LATEX: \renewcommand{\y}{{\bf y}}
#+LATEX: \renewcommand{\ys}{{\bf y}^{*}}
#+LATEX: \renewcommand{\yp}{{\bf y}^{\prime}}
#+LATEX: \renewcommand{\ysp}{{\bf y}^{*\prime}}
#+LATEX: \renewcommand{\yh}{\hat{{\bf y}}}
#+LATEX: \renewcommand{\yhp}{\hat{{\bf y}}^{\prime}}
#+LATEX: \renewcommand{\In}{{\bf I}_n}
#+LATEX: \renewcommand{\sigs}{\sigma^{2}}
#+LATEX: \setlength{\parindent}{0in}
#+STARTUP: fninline

\textbf{Generalized Least Squares and} \texttt{ggplot} \hfill
*ARE212*: Section 07 \\ \\

This section will briefly outline two general concepts, GLS and
=ggplot=.  As an addendum, we will go over the recurring =R= questions
that cropped up during the midterm.  We will examine the
characteristics of generalized least squares (GLS), and specifically
the efficiency gains from a special case of GLS, weighted least
squares (WLS).  This is the econometric concepts.  We will then
recreate the graphs from Figures 2.6 and 2.7, roughly, in the notes
using =ggplot2= a very popular graphing package in =R=.  This is part
is optional, especially since it is only a very brief treatment of the
package --- there is a lot more to learn. \\

Let $x \sim U(0,2000)$ and $\epsilon \sim N(0,(x/1000)^2)$.  The
underlying data generating process in (2.102) is $y_i = \alpha + x_i
\beta + \epsilon$, where $\alpha = 0.5$ and $\beta = 1.5$.  The
objective is to plot the simulated sampling distribution of the OLS
estimator applied to $B = 10,000$ draws, each of size $n = 1000$.
First, let's generate the sample data for one draw.

#+begin_src R :results output graphics :exports both :tangle yes :session
  n <- 1000
  x <- runif(n, min=0, max=2000)
  eps <- rnorm(n, 0, sqrt((x/1000)^2))
  y <- 0.5 + x*1.5 + eps
#+end_src

#+RESULTS:

Now we can calculate the standard OLS parameter vector $[\ah
\hspace{6pt} \bh]^{\prime}$ by noting that $\X$ is just the $x$ vector
bound to a column of ones.  We will only examine $\hat{\beta}$ for
this section, rather than both parameters.

#+begin_src R :results output graphics :exports both :tangle yes :session
  X <- cbind(1, x)
  params <- solve(t(X) %*% X) %*% t(X) %*% y
  beta <- params[2]
  print(beta)
#+end_src

#+RESULTS:
: [1] 1.50002

Let's package this into a function, called =rnd.beta=, so that we can
collect the OLS parameter for an arbitrary number of random samples,
noting that $n$ is a constant so we may as well keep it out of the
function so that $1000$ is not reassigned thousands of times to $n$.

#+begin_src R :results output graphics :exports both :tangle yes :session
  rnd.beta <- function(i) {
    x <- runif(n)
    eps <- rnorm(n, 0, sqrt(x/10))
    y <- 0.5 + x * 1.5 + eps
    X <- cbind(1, x)
    params <- solve(t(X) %*% X) %*% t(X) %*% y
    beta <- params[2]
    return(beta)
  }
#+end_src

#+RESULTS:

Since there aren't any supplied arguments, the function will return an
estimated $\bh$ from a a different random sample for each call:

#+begin_src R :results output graphics :exports both :tangle yes :session
  rnd.beta()
  rnd.beta()
#+end_src

#+RESULTS:
: [1] 1.471441
: [1] 1.486697

This is convenient for bootstrapping without loops, but rather
applying the function to a list of effective indices.[fn:: This is
much more comfortable for me, with a background in functional
programming.  There is some inherent value, however, in keeping code
compact by mapping across indices rather than incrementing an index
within a =for= loop; the code is more readable and less prone to
typos.} Now replicating the process for $B$ draws is straightforward:

#+begin_src R :results output graphics :exports both :tangle yes :session
  B <- 100
  beta.vec <- sapply(1:B, rnd.beta)
  mean(beta.vec)
#+end_src

#+RESULTS:
: [1] 1.501141

Alright.  Looking good.  The average of the simulated sample is much
closer to $\beta$ than any individual call of =rnd.beta=, suggesting
that the distribution of the simulated parameters will be
appropriately centered.  Now, let's create another, similar function
that returns the WLS estimates.

#+begin_src R :results output graphics :exports both :tangle yes :session
  rnd.wls.beta <- function(i) {
    x <- runif(n)
    y <- 0.5 + x * 1.5 + rnorm(n, 0, sqrt(x / 10))
    C <- diag(1 / sqrt(x / 10))
    y.wt <- C %*% y
    X.wt <- C %*% cbind(1, x)
    param.wls <- solve(t(X.wt) %*% X.wt) %*% t(X.wt) %*% y.wt
    beta <- param.wls[2]
    return(beta)
  }
  wls.beta.vec <- sapply(1:B, rnd.wls.beta)
#+end_src

#+RESULTS:

We now have two collections of parameter estimates, one based on OLS
and another based on WLS.  It is straightforward to plot two separate
histograms using =R='s core histogram plotting function =hist()=.
However, we can use this to introduce a more flexible, powerful
graphing package called =ggplot2=.  

#+CAPTION: Relative efficiency of WLS
#+LABEL: fig:dens
#+begin_src R :results output graphics :file inserts/hist.png :width 700 :height 400 :session :tangle yes :exports both 
  library(ggplot2)
  labels <- c(rep("ols", B), rep("wls", B)) 
  data <- data.frame(beta=c(beta.vec, wls.beta.vec), method=labels)
  ggplot(data, aes(x=beta, fill=method)) + geom_density(alpha=0.2)
#+end_src

#+RESULTS:
[[file:inserts/hist.png]]

* A review of loops, a loopy review

There were many questions about =R= code on the midterm, and
especially code that will traverse a sequence of indices.  There are
many ways to loop over a sequence and iterate on a function --- and we
do this a lot in ARE212 to examine the behavior of different
estimators.  I used =sapply()= to iterate over a sequence.  There are
other ways to do this.  Let's consider an equivalent for-loop.

#+begin_src R :results output graphics :exports both :tangle yes :session
  res <- rep(NA, B)
  for (i in seq(B)) {
    res[i] <- rnd.wls.beta()
  }
  print(head(res))
#+end_src

#+results:
: [1] 1.489108 1.491555 1.511507 1.500539 1.499187 1.513985

This is fine.  It works.  You have to pay attention, however, to the
expressed indices instead of using them for implicit increments; and
you have to preallocate a results vector which also can get confusing
over large, sprawling projects.  Also, there are more lines of code
(LOC) which is often used as a metric for inefficiency.  \\

Another thing that might help avoid loops is something called
/vectorized operations/.  Instead of looping through indices, is there
a way to formulate the problem in terms of element-wise operations?
Suppose, for example, that I want to generate a binary vector that
indicates whether the value in one variable exceeds the value in
another.  

#+begin_src R :results output graphics :exports both :tangle yes :session
  x <- rnorm(10); y <- rnorm(10)
  (D <- ifelse(x > y, 1, 0))
#+end_src  

#+RESULTS:
:  [1] 0 1 1 0 0 0 0 1 1 0

The function =ifelse()= operates on each individual element of the
equal-length =x= and =y= vectors.  No looping necessary.  This comes
up a lot; and you should definitely make use of (and document) this
behavior when available.  This doesn't always work, however,
especially when we are trying to generate a random vector.  Suppose we
want to create a vector where the elements are pulled from different
distributions, depending on the value of =D=.

#+begin_src R :results output graphics :exports both :tangle yes :session
  factor(x <- ifelse(D == 1, runif(1), rnorm(1)))
#+end_src  

#+RESULTS:
:  [1] 0.146645265993786   0.00228817900642753 0.00228817900642753
:  [4] 0.146645265993786   0.146645265993786   0.146645265993786  
:  [7] 0.146645265993786   0.00228817900642753 0.00228817900642753
: [10] 0.146645265993786  
: Levels: 0.00228817900642753 0.146645265993786

What happened?  There are only two values in this vector.  The random
value was generated before the vectorized boolean check.  This is
equivalent, then, to the code by which we generated =D= in the first
place.  To fix this problem, we may have to apply the function to each
value within the reference vector:

#+begin_src R :results output graphics :exports both :tangle yes :session
  randme <- function(d) {
    if (d == 1) {
      r <- runif(1)
    } else {
      r <- rnorm(1, mean = 100, sd = 19)
    }
    return(r)
  }
  
  factor(x <- sapply(D, randme))
#+end_src

#+RESULTS:
:  [1] 0.545478682033718 97.0212761082557  0.589886064874008 0.781325699063018
:  [5] 108.67800788386   0.868332070531324 54.3405163802937  0.810937537113205
:  [9] 0.857683679787442 101.00935385884  
: 10 Levels: 0.545478682033718 0.589886064874008 ... 108.67800788386

: 
