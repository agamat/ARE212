#+INCLUDE: ../orgpreamble.org

*Non-standard standard errors* \hfill
*ARE212*: Section 07 \\ \hline \bigskip

This week we'll be talking about various methods of computing functions of standard errors. We'll begin by discussing how to compute the standard error of a sum of two coefficients, followed by the more complex standard error of the prediction. Finally, we'll show how to compute nonlinear functions of coefficients.

* Standard error of a linear combination of coefficients
Let's return to section 5 to think about wages. Suppose we run the following model:
$$ \text{wage}_i = \alpha + \beta_1 \text{EDUC}_i + \beta_2 \text{TENURE}_i + \varepsilon_i $$

We can run this model in =R= as follows (after, of course, setting up our OLS function):

#+begin_src R :results output latex :exports both :tangle yes :session
OLS <- function(y,X) {
  n <- nrow(X); k <- ncol(X)
  b <- solve(t(X) %*% X) %*% t(X) %*% y
  e <- y - X %*% b; s2 <- t(e) %*% e / (n - k); XpXinv <- solve(t(X) %*% X)
  se <- sqrt(s2 * diag(XpXinv))
  t <- b / se
  p <- 2 * pt(-abs(t),n-k)
  output <- data.frame(b, se, t, p)
  colnames(output) <- c("Estimate","Std. Error", "t statistic", "p-value")
  return(output)
}

library(foreign)
library(xtable)
#data <- read.dta("http://fmwww.bc.edu/ec-p/data/wooldridge/wage2.dta")
data <- read.dta("wage2.dta")
data <- data[ , c("wage", "educ", "tenure")]
data <- na.omit(data)
y <- data$wage
X <- cbind(1,data$educ,data$tenure)
OLS.out <- OLS(y,X)
xtable(OLS.out)
#+end_src

#+RESULTS:
#+BEGIN_LaTeX
% latex table generated in R 3.0.3 by xtable 1.7-3 package
% Mon Mar 10 23:55:04 2014
\begin{table}[ht]
\centering
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t statistic & p-value \\
  \hline
1 & 53.52 & 79.56 & 0.67 & 0.50 \\
  2 & 61.15 & 5.64 & 10.84 & 0.00 \\
  3 & 11.18 & 2.44 & 4.58 & 0.00 \\
   \hline
\end{tabular}
\end{table}
#+END_LaTeX

Great!

But suppose the actual coefficients aren't what you actually care about. Maybe what you /really/ want to compute is the additional wage benefit[fn:: Here I ask you to make the appropriate suspension of disbelief w.r.t the causal nature of this model.] of three years of education and two years at a company. Let's call it $d = 3 b_1 + 2 b_2$. Kind of weird, but easy enough:

#+begin_src R :results output graphics :exports both :tangle yes :session
(d <- 3 * OLS.out[2,1] + 2 * OLS.out[3,1])
#+end_src

#+RESULTS:
: [1] 205.7979

However, this isn't really a meaningful answer without standard errors. How do we get those? If we were using =Stata=, the immediate answer is =lincom=. And in fact, there are canned functions in =R= that can do the same thing. But it's more interesting to do it by hand, and it gives us the opportunity to demonstrate three different ways of computing standard errors.

** Analytical method
The first way is the analytical method. Here, it's fairly straightforward to compute the variance of $d$ using the properties of the variance formula:
\begin{align*}
V(d) &= V(3 b_1 + 2 b_2) \\
V(d) &= 9 V(b_1) + 4 V( b_2) + (3 \times 2) \text{ Cov}(b_1,b_2)\\
\end{align*}
Fortunately, we can easily get estimates for $V(b_1)$, $V(b_2)$, and $\text{Cov}(b_1,b_2)$ by computing the variance-covariance matrix of the coefficients: $V(\b) = s^2 (\X' X)^{-1}$. They will be the entries in (2,2), (3,3), and (2,3), respectively. To make this easier, we'll write a function that returns this matrix.

#+begin_src R :results output graphics :exports both :tangle yes :session
get.vcov <- function(y, X)  {
  n <- nrow(X); k <- ncol(X)
  b <- solve(t(X) %*% X) %*% t(X) %*% y
  e <- y - X %*% b
  s2 <- as.vector(t(e) %*% e / (n - k))
  XpXinv <- solve(t(X) %*% X)
  vcov <- s2 * XpXinv
  return(vcov)
}
vcov <- get.vcov(y,X)
var.b1 <- vcov[2,2]
var.b2 <- vcov[3,3]
cov.b1b2 <- vcov[2,3]
(d.se <- sqrt(9 * var.b1 + 4 * var.b2 + 3*2*cov.b1b2))
#+end_src

#+RESULTS:
: [1] 17.69076

Great! We now know that our estimate of $d$ is 205.8 with a standard error of 17.7. We can verify this in =R= using the =estimable()= command in the =gmodels= package (you may have to install it).

#+begin_src R :results output graphics :exports both :tangle yes :session
library(gmodels)
lm <- lm(data$wage ~ data$educ + data$tenure)
hm <- c(0,3,2)
estimable(lm, hm)
#+end_src

#+RESULTS:
:         Estimate Std. Error  t value  DF Pr(>|t|)
: (0 3 2) 205.7979   17.77496 11.57797 932        0

** Delta method
Let $\A(\betab) \equiv \frac{\partial \ab (\betab)}{\partial \betab'}$. From Max's notes, we know the following:
$$ \sqrt{N}(\x_N - \betab) \xrightarrow{d} N(\bf{0}, \Sigmab) \Rightarrow \sqrt{N} (\ab (\x_N) - \ab (\betab)) \xrightarrow{d} N(\bf{0}, \A (\betab) \Sigma \A} (\betab)')$$

This looks intimidating, but it's actually fairly straightforward to bend it to our setting. Let $\x_N \equiv \b$ and $\ab(\x_N) \equiv d(\betab)$.  This gives us that $\A \equiv \D(\betab) = [ 0 \hspace{5pt} 3\hspace{5pt} 2 ]}$.\\

We know from the proof in section 4.2 of the lecture notes that $\sqrt{N}(\b - \betab) \xrightarrow{d} N(\bf{0}, \sigs (\X'\X)^{-1})$, so we immediately get that $V(d) \approx \D \sigs (\X'\X)^{-1} \D'$. So to approximate the variance of our linear combination of coefficients, $d$, all we have to do is pre- and post-multiply $\sigs (\X'\X)^{-1}$ by the gradient of $d$, $\D$. As usual, the standard error is just the square root of the variance. Putting it all together gives us our answer:

#+begin_src R :results output graphics :exports both :tangle yes :session
D <- matrix(c(0,3,2), ncol = 3)
(sqrt(D %*% vcov %*% t(D)))
#+end_src

#+RESULTS:
:          [,1]
: [1,] 17.77496

Wow! It works! I'm as surprised as you are.

** Bootstrapping the standard errors

* Computing the standard error of the prediction
The standard error the prediction is an important quantity for econometricians who are interested in forecasting. It answers the question "how much variation should I expect to see when I use the coefficients from OLS to predict other values of y?" Modern statistical software has built-routines to calculate the prediction and the standard error of the prediction, but it's useful to know the calculation is performed. In fact, since we're using linear models, the prediction is just another linear combination!

Our model is
$$ \y = \X \b + \eps $$
We want to compute $V(e^0) = V(\hat{y}^0 - y^0)$, the variance of the error in prediction for a particular set of covariates $\x^0$.
** Analytical
First, we can calculate the variance analytically. This is similar in spirit to the analytical exercise above, but since we are working with more complex objects (coefficients, data, and disturbances), we'll use matrix notation. Still, everything that follows is just algebra and the use of the properties of the variance function.
\begin{align*}
V(\hat{y}^0 - y^0) & = V(\X^0 \b - \X^0 \betab - \eps^0) \\
& = \sigs + V(\X^0 ( \b - \betab)) \\
& = \sigs + V(\X^0 \b ) \\
& = \sigs + \X^0 V(\b) \Xp^0 \\
& = \sigs + \X^0 \sigs (\Xp \X)^{-1} \Xp^0 \\
& = \sigs( 1 + \X^0 (\Xp \X)^{-1} \Xp^0 )
\end{align*}

Of course, using the $y^0$ and $\X^0$ is only to make the point that we are actually computing the standard error of the prediction for a single set of data $\X^0$. Practically, we can tell =R= to compute the standard errors for all of observations all at once. We'll demonstrate using the =iris= dataset built into =R= and attempt to predict sepal length from sepal width and petal width[fn:: Dammit Jim, I'm a doctor, not a botanist!].

#+begin_src R :results output graphics :exports both :tangle yes :session
iris.df <- iris
y <- iris.df$Sepal.Length
X <- cbind(1, iris.df$Sepal.Width, iris.df$Petal.Width)
b <- OLS(y,X)[ ,1]
head(pred.y <- X %*% b)
#+end_src

#+RESULTS:
:          [,1]
: [1,] 5.048507
: [2,] 4.848972
: [3,] 4.928786
: [4,] 4.888879
: [5,] 5.088414
: [6,] 5.402561

Getting the predictions for our data $\X$ is the easy part. Now we want to get some estimate of the standard error on these predictions. We'll do so using the formula we derived above:

#+begin_src R :results output graphics :exports both :tangle yes :session
e <- y - X %*% b
n <- nrow(X); k <- ncol(X)
s2 <- as.vector(t(e) %*% e / (n - k))
XpXinv <- solve(t(X) %*% X)
pred.se <- diag(sqrt(s2 * (1 + X %*% XpXinv %*% t(X))))
head(pred.se)
#+end_src

#+RESULTS:
: [1] 0.4556677 0.4558185 0.4552114 0.4554239 0.4561838 0.4583894

To verify, we can use =lm()= and =predict()=:

#+begin_src R :results output graphics :exports both :tangle yes :session
lm <- lm(Sepal.Length ~ Sepal.Width + Petal.Width, data = iris.df)
predict.out <- predict(lm, se.fit = T)
all.equal(as.vector(predict.out$fit),as.vector(pred.y))
all.equal(as.vector(predict.out$se.fit),as.vector(pred.se))
#+end_src

#+RESULTS:
: [1] TRUE
: [1] "Mean relative difference: 6.394198"

Something's not right. Let's investigate.

#+begin_src R :results output graphics :exports both :tangle yes :session
head(predict.out$se.fit)
head(pred.se)
#+end_src

#+RESULTS:
: [1] 0.06446854 0.06552559 0.06116029 0.06272227 0.06802035 0.08151061
: [1] 0.4556677 0.4558185 0.4552114 0.4554239 0.4561838 0.4583894

Hm. That looks fishy.  I'll spare you the suspense --- =R= apparently has a different definition of the standard error of the prediction, and is returning the square root of $V(\hat{y}^0)$, rather than the square root of $V(\hat{y}^0 - y^0)$. This is why you should be careful with canned results! Anyway --- we can replicate what they have easily:
#+begin_src R :results output graphics :exports both :tangle yes :session
pred.se.2 <- sqrt(diag(s2 * (X %*% XpXinv %*% t(X))))
head(pred.se.2)
all.equal(as.vector(predict.out$se.fit),as.vector(pred.se.2))
#+end_src

#+RESULTS:
: [1] 0.06446854 0.06552559 0.06116029 0.06272227 0.06802035 0.08151061
: [1] TRUE

Whew. Mystery solved. Note that =predict()= /does/ know how to return the 95% prediction interval, and will do so if we set interval = "prediction".

** Delta method
To use the delta method, we'll use that $V(\hat{y}^0 - y^0) = \sigs + V(\X^0 \b )$. Now we'll use the delta method to compute V(\X^0 \b). Let $\ab \equiv X^0 \b$ and $\A \equiv X^0$. This gives us the following familiar result:
$$ V(\hat{y}^0 - y^0) = \sigs + \X^0 \sigs (\X'\X)^{-1} \Xp^0  = \sigs( 1 + \X^0 (\Xp \X)^{-1} \Xp^0 )$$

In this case, the "approximation" provided by the delta method is really the true variance that we calculated earlier using our analytical method! This is because the delta method uses a first-order Taylor approximation, which is only an approximation when our function $\ab$ has second-order terms.
* Non-linear functions of coefficients
So far, we've only computed linear functions of coefficients. That's pretty cool, but suppose we're interested in some /non-linear/ function of the coefficients. How do we compute the standard errors? Let's take an example[fn:: Credit for this example goes to Dason Kurkiewicz, whose post at http://dasonk.github.io/r/2013/02/09/Using-the-delta-method/ inspired this section.]. \\

Suppose we are running a quadratic regression model:
$$y_i = \alpha + \beta_1 x_i + \beta_2 x_i^2 + \varepsilon_i$$

First, we'll simulate some data.

#+begin_src R :results output graphics :exports both :tangle yes :session
set.seed(42)
n <- 50
x <- runif(n,0,8)
x2 <- x^2
eps <- rnorm(n,0,5)
X <- cbind(1, x, x2)
beta <- c(3, 8, -1)
y <- X %*% beta + eps
b <- OLS(y,X)[ ,1]
#+end_src

#+RESULTS:

We'll graph this using a quadratic fit line. This is cheating, since normally we'd look at the scatterplot of our data, see the quadratic trend, and /then/ fit the line, but you get the idea.

#+begin_src R :results output graphics :file inserts/fig1.png :width 600 :height 300 :session :tangle yes :exports both
library(ggplot2)
curve.df <- data.frame(y,x)
(g <- ggplot(data = curve.df, aes(x=x, y=y)) + geom_point()
      + geom_smooth(method="lm", formula = y ~ x + I(x^2)))
#+end_src

#+RESULTS:
[[file:inserts/fig1.png]]

This is great. We specified a quadratic model in our simulation and a quadratic line seems to fit!  But what we /really/ want to know is the value of $x$ for which the maximum of the line is reached. Computing this for a regression is easy enough: the maximum value will always occur at $x = \frac{-\beta_1}{2\beta_2}$. Let's plot this again, but this time we'll draw two vertical lines: one that shows the true $x$ that gives the maximum value (spoiler alert: it's $x_{max}=4$) and one that shows the one we can compute using the formula $x_{max} = \frac{-b_1}{2*b_2}$.

#+begin_src R :results output graphics :file inserts/fig2.png :width 600 :height 300 :session :tangle yes :exports both
true.xmax <- -beta[2] / (2 * beta[3])
est.xmax <- -b[2] / (2 * b[3])
(g<- g + geom_vline(xintercept = trueint, colour = "green", size=1)
   + geom_vline(xintercept = estint, colour = "red", size=1))
#+end_src

#+RESULTS:
[[file:inserts/fig2.png]]


Due to the noise of the data, we haven't quite approximated the true maximum (although this is actually quite good for 50 observations with plenty of variance!). But what about the standard errors?

** Analytically
It is, in principle, possible to compute the standard errors of $x_{max} = -\frac{b_1}{2b_2}$ analytically. We can think of the components of our coefficient vector $\b$ as random variables themselves (otherwise how would we be getting their variances), so all $V(x_{max})$ is just the variance of the ratio of two random variables. But this is where I get off the train --- the formula is totally horrible[fn:: You really want to know? See "ON THE ARITHMETIC MEANS AND VARIANCES
OF PRODUCTS AND RATIOS OF RANDOM VARIABLES" by Fred Fishman, 1971.]. No more will be said of it.

** Delta method
Forget using the delta method for sums of variables. Where it /really/ shines is in enabling lazy econometricians like myself easily calculate the standard errors of products and quotients (or worse!) of coefficients. As usual, we let $\ab(\beta) = x_{max}(\beta_1, \beta_2) = -\frac{\beta_1}{2 \beta_2}$, which gives that $\A = [-\frac{1}{\beta_2} \hspace{5pt} \frac{\beta_1}{\beta_2^2}]'$. This gives us our delta method approximation of $V(x_{max}) = \A V(\beta) \Ap = \sigs \A (\X'\X)^{-1} \Ap$. As usual, we'll replace the population parameters with their sample equivalents.

Computationally, this is straightforward:

#+begin_src R :results output graphics :exports both :tangle yes :session
b1 <- b[2]; b2 <- b[3]
A <- matrix(c(0, -1 / (2 * b2), b1 / (2* b2^2)), nrow = 1)
vcov <- get.vcov(y,X)
(se.xmax <- sqrt(A %*% vcov %*% t(A)))
#+end_src

#+RESULTS:
:          [,1]
: [1,] 0.191088

Just to make sure we're doing this right, we'll compute the standard errors using the =deltamethod= function from the =msm= package (which you may need to install):

#+begin_src R :results output graphics :exports both :tangle yes :session
o <- lm(y ~ x + I(x^2))
library(msm)
(standerr <- deltamethod(~-x2/(2 * x3), coef(o), vcov(o)))
#+end_src

#+RESULTS:
: [1] 0.191088

*Kablammo!* For the coup de grace, we'll graph this again, this time with the 95% confidence intervals around our estimate. This is a good time to marvel at how nice it is to progressively build graphs in =ggplot2=.

#+begin_src R :results output graphics :file inserts/fig3.png :width 600 :height 300 :session :tangle yes :exports both
ci <- est.xmax + c(-1, 1) * qt(0.975, n-k) * se.xmax
(g + geom_vline(xintercept = ci[1], colour = "red", size=1, linetype="dashed", alpha=0.5)
   + geom_vline(xintercept = ci[2], colour = "red", size=1, linetype="dashed", alpha=0.5))
#+end_src

#+RESULTS:
[[file:inserts/fig3.png]]

*KAPOWEE!* The 95% confidence interval contains the true estimate! And we're done.
