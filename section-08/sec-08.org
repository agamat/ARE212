#+INCLUDE: ../orgpreamble.org

*Asymptopia is the best -topia* \hfill
*ARE212*: Section 08 \\ \hline \bigskip

In this section we'll briefly clarify some concepts from last time, and then we'll build up a simulation in =R= to show why living in asymptotic world is great.

* The difference between prediction and confidence intervals

Last week I punted on demonstrating the difference between prediction and confidence intervals. It turns out that we had already demonstrated the difference between the two of them, I just didn't label them correctly. To remind you, we had the following population model:
$$ \y = \X \b + \eps $$
We computed that the variance of the /individual prediction/, $V(\hat{y}^0 - y^0)$ and the variance of the /mean prediction/, $V(\hat{y}^0)$, and showed they are different estimates[fn:: Simon Jackman has a nice, complete exploration of this topic here: http://jackman.stanford.edu/classes/350B/07/predictionforWeb.pdf.]:

\begin{align}
V(\hat{y}^0 - y^0) &= \sigs( 1 + \X^0 (\Xp \X)^{-1} \Xp^0 ) \label{vpe} \\
V(\hat{y}^0) &= \sigs(\X^0 (\Xp \X)^{-1} \Xp^0 ) \label{vp}
\end{align}

It turns out that when we compute the /prediction interval/ (i.e. the interval around the individual prediction), we use (\ref{vpe}), while when we compute the /confidence interval/ (i.e. the interval around the mean prediction), we use (\ref{vp}). Hopefully this terminology makes sense to you. The intution is that the prediction interval tells us the error we should expect when we predict /new/ values of $y$, whereas the confidence interval tells us the confidence we should have in the estimate $\hat{y}^0 = \X^0 \b$. \\

The source of the confusion was that =predict()=, the canned =R= command, returns the standard error of the prediction (i.e. the square root of (\ref{vp})), when we set =se.fit = TRUE=. But we can also use =interval= setting to return both types of intervals! Let's dive into =R= to demonstrate a key fact about the relationship between prediction and confidence intervals. First, we'll build up our simulated data:

#+begin_src R :results output :exports both :tangle yes :session
OLS <- function(y,X) { b <- solve(t(X) %*% X) %*% t(X) %*% y }
set.seed(42)
n <- 200
X <- cbind(1, rnorm(n))
beta <- c(2, 3)
eps <- rnorm(n)
y <- X %*% beta + eps
#+end_src

#+RESULTS:

Now we can construct both the standard errors for the prediction interval and the confidence interval. We'll name them intuitively.

#+begin_src R :results output :exports both :tangle yes :session
b <- OLS(y,X)
pred.y <- X %*% b
e <- y - pred.y
n <- nrow(X); k <- ncol(X)
s2 <- as.vector(t(e) %*% e / (n - k))
XpXinv <- solve(t(X) %*% X)
pred.se <- sqrt(diag(s2 * (1 + X %*% XpXinv %*% t(X))))
conf.se <- sqrt(diag(s2 * (0 + X %*% XpXinv %*% t(X))))
(cbind(head(pred.se), head(conf.se)))
#+end_src

#+RESULTS:
:           [,1]       [,2]
: [1,] 0.9536325 0.11724104
: [2,] 0.9494816 0.07645859
: [3,] 0.9491422 0.07212048
: [4,] 0.9498495 0.08089918
: [5,] 0.9492266 0.07322351
: [6,] 0.9487766 0.06713905

As with our coefficient estimates, we'll calculate the confidence intervals using the 95% critical value from a $t_{n-k}$ distribution.

#+begin_src R :results output :exports both :tangle yes :session
crit.t <- qt(0.975,n-k)
confint.t <- cbind(pred.y - crit.t * conf.se, pred.y + crit.t * conf.se)
predint.t <- cbind(pred.y - crit.t * pred.se, pred.y+ crit.t * pred.se)
#+end_src

#+RESULTS:

We could have also calculated and checked these using =predict()=. But they're right. Trust me. Now, let's graph these to see what we have!

#+begin_src R :results output graphics :file inserts/fig1.png :width 800 :height 500 :session :tangle yes :exports both
draw.df <- data.frame(X[ ,2], y, pred.y, confint.t, predint.t)
names(draw.df) <- c("x", "y", "pred.y","conf.low","conf.high","pred.low","pred.high")
library(ggplot2)
(g <- ggplot(data = draw.df, aes(x = x, y = y)) + ggtitle("Prediction vs. Confidence Intervals") +
  geom_ribbon(aes(ymin = pred.low, ymax = pred.high), fill = "chartreuse1", linetype = 0) +
  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), fill = "coral1", linetype = 0) +
  geom_point(colour = "darkslategray", size = 0.2))
#+end_src

#+RESULTS:
[[file:inserts/fig1.png]]


Of course... we could also have just looked at the two formulas and noted that $\sigs > 0$ to see that $V(\hat{y}^0 - y^0) > V(\hat{y}^0)$.

* Strict Exogeneity vs. Population orthogonality		   :noexport:

I have a little bit of code for this... but I'm not sure if it's worth doing.

* The glory of asymptopia

Finally, the main event. In this section, we'll verify in code what we showed in section 4.2 in lecture. We'll demonstrate that, given linearity, population orthogonality, and our asymptotic full rank condition, no matter what distribution on the disturbances, the estimated $\b$ will be distributed normal as $N \rightarrow \infty$. \\

We'll be doing another simulation exercise. Let's get started. First, we'll define our true population of interest, the size of sample for each draw, and the number of draws.

#+begin_src R :results output :exports both :tangle yes :session
pop.n <- 10000
n <- 500
draws <- 1000
pop.X <- cbind(1, runif(pop.n,0,20))
#+end_src

#+RESULTS:

Next we want to define some errors. To make it interesting, we'll do this whole exercise with four different sets of errors. The sets will be distributed normal, uniform, poisson, and finally a crazy bimodal gamma distribution that I made up. This is going to be awesome.

#+begin_src R :results output :exports both :tangle yes :session
bimodalDistFunc <- function (n, cpct, mu1, mu2, sig1, sig2) {
  d0 <- rgamma(n, 1) + 3
  d1 <- rgamma(n, 1)
  flag <- rbinom(n, size = 1, prob = cpct)
  d <- d1 * (1 - flag) + d0 * flag
}

pop.eps.0 <- rnorm(pop.n)
pop.eps.1 <- runif(pop.n, -5, 5)
pop.eps.2 <- rpois(pop.n, 1) - 1
pop.eps.3 <- bimodalDistFunc(n=pop.n, cpct = 0.7, mu1 = 2, mu2 = 10, sig1 = 1, sig2 = 2) - 3.1
#+end_src

#+RESULTS:

It's important to note that I rigged these epsilons so that they all have mean zero:

#+begin_src R :results output :exports both :tangle yes :session
c(mean(pop.eps.0), mean(pop.eps.1), mean(pop.eps.2), mean(pop.eps.3))
#+end_src

#+RESULTS:
: [1] -0.005288162  0.006531189  0.027400000 -0.003300807

This isn't strictly necessary, but it makes our lives easier down the road. Now we'll create a data frame with our errors that we can easily graph using =ggplot2=. This takes a little doing --- we need to stack the errors on top of each other and add a factor variable for the type of error they are. Factor variables are just categorical variables in =R=.

#+begin_src R :results output :exports both :tangle yes :session
pop.eps <- c(pop.eps.0, pop.eps.1, pop.eps.2, pop.eps.3)
pop.eps.type <- c(rep("Normal",pop.n), rep("Uniform", pop.n), rep("Poisson", pop.n), rep("Bimodal Gamma", pop.n))
class(pop.eps.type)
pop.eps.type <- factor(pop.eps.type, levels = c("Normal", "Uniform", "Poisson", "Bimodal Gamma"))
levels(pop.eps.type)
class(pop.eps.type)
pop.eps.df <- data.frame(pop.eps, pop.eps.type)
#+end_src

#+RESULTS:
: [1] "character"
: [1] "Normal"        "Uniform"       "Poisson"       "Bimodal Gamma"
: [1] "factor"

Now that we've set all of this up, why not see what these crazy errors look like?

#+begin_src R :results output graphics :file inserts/fig2.png :width 800 :height 500 :session :tangle yes :exports both
(g <- ggplot(data = pop.eps.df, aes(x = pop.eps)) + geom_histogram(aes(y=..density..)) + facet_wrap( ~ pop.eps.type, ncol=2, scales = "free"))
#+end_src

#+RESULTS:
[[file:inserts/fig2.png]]

Cool. The last three all violate our normality assumption pretty egregiously. Let's see how they do... in *asymptopia*! \\

Now we'll define our population outcome variables, one for each set of errors:
#+begin_src R :results output :exports both :tangle yes :session
beta <- c(4,2)
pop.y.0 <- pop.X %*% beta + pop.eps.0
pop.y.1 <- pop.X %*% beta + pop.eps.1
pop.y.2 <- pop.X %*% beta + pop.eps.2
pop.y.3 <- pop.X %*% beta + pop.eps.3
#+end_src

#+RESULTS:

We also need to build a function that returns OLS estimates for a given set of population variables. =getb= will be that function, and the code it in should be familiar from section 6. As we did then, we'll randomply sample indices (without replacement) and then pull the corresponding data from =pop.x= and =pop.y=.

#+begin_src R :results output :exports both :tangle yes :session
getb <- function(draw, pop.n, pop.y, pop.X) {
  indices <- sample(1:pop.n, n)
  y <- pop.y[indices]
  X <- pop.X[indices, ]
  b <- OLS(y,X)
  return(b)
}
#+end_src

#+RESULTS:

The code above won't win any points for efficiency, since we're passing the whole population every time, but it does generalize pretty well. In fact, we'll only modify the population of =y= that we pass.

#+begin_src R :results output :exports both :tangle yes :session
blist.0 <- t(sapply(1:draws, getb, pop.n = pop.n, pop.y = pop.y.0, pop.X = pop.X))
blist.1 <- t(sapply(1:draws, getb, pop.n = pop.n, pop.y = pop.y.1, pop.X = pop.X))
blist.2 <- t(sapply(1:draws, getb, pop.n = pop.n, pop.y = pop.y.2, pop.X = pop.X))
blist.3 <- t(sapply(1:draws, getb, pop.n = pop.n, pop.y = pop.y.3, pop.X = pop.X))
cbind(head(blist.0),head(blist.1),head(blist.2),head(blist.3))
#+end_src

#+RESULTS:
:          [,1]     [,2]     [,3]     [,4]     [,5]     [,6]     [,7]     [,8]
: [1,] 3.950786 2.002674 3.776555 2.008685 4.237680 1.986850 4.243113 1.987808
: [2,] 3.902267 2.009341 4.142603 1.974937 3.937415 1.999047 4.129825 1.998343
: [3,] 3.942421 2.004543 3.910602 1.999602 3.987099 2.004227 4.206357 1.988775
: [4,] 3.811743 2.011721 4.235077 1.949349 3.966318 2.015415 3.970238 2.002788
: [5,] 4.091621 1.987538 4.210565 1.993504 4.025247 1.998070 4.278379 1.977353
: [6,] 4.009503 1.996708 3.943357 2.001888 4.036146 2.010304 3.984694 2.000783

This looks pretty good so far. The odd columns are our point estimates for $\alpha$, the intercept, and the even columns are our estimates for $\beta_1$, the coefficient estimate. From now on we'll focus on $\beta_1$. We've generated a huge amount of these point estimates, but now we'd like to see if they're truly distributed normaly. To do this, we'll create a function that graphs each of them.

#+begin_src R :results output :exports both :tangle yes :session
make.plots <- function(blist) { # makehist returns a graph
  blist.df <- data.frame(blist)
  names(blist.df) <- c("alpha", "beta_1")
  estmean <- mean(blist.df$beta_1)
  estsd <- sd(blist.df$beta_1)
  g <- ggplot(data = blist.df, aes(x = beta_1)) +
   geom_histogram(aes(y=..density..), fill="blue", colour="white", alpha=0.3) +
   geom_density(colour = "black", size = 1, linetype = "dashed") +
   stat_function(geom="line", fun=dnorm, arg=list(mean = estmean, sd = estsd), colour = "red", size = 1, linetype = "dashed")
  return(g)
}

g0 <- make.plots(blist.0) + ggtitle("Normal errors")
g1 <- make.plots(blist.1) + ggtitle("Uniform errors")
g2 <- make.plots(blist.2) + ggtitle("Poisson errors")
g3 <- make.plots(blist.3) + ggtitle("Bimodal gamma errors")
#+end_src

#+RESULTS:

Our function =makehist= returns a graph a histogram, smoothed density, and a normal curve with the same mean and standard deviation as the data, so we just run it 4 times to get our 4 graphs (one for each list of our estimated $b$ values). Next, we'll use the =gridExtra= package (which you may need to install) to display the graphs together, just as we did with our errors. Note that we use =gridExtra= and not the =facets= command as we did before just because =facets= wasn't displaying the normal distribution correctly. What we'd like to see is that our smoothed density function and histogram look roughly similar to the normal distribution. Cross your fingers!

#+begin_src R :results output graphics :file inserts/fig3.png :width 800 :height 500 :session :tangle yes :exports both
library(gridExtra)
grid.arrange(g0, g1, g2, g3, ncol = 2)
#+end_src

#+RESULTS:
[[file:inserts/fig2.png]]

Hey, these actually look pretty good! But this isn't a rigorous test. There are actually statistical methods available to test whether a distribution is normal or not. We'll use one called the Shapiro-Wilk's test of normality. Unfortunately, the computation of Shapiro-Wilk's is a bit complicated, so we'll be leaning on a canned command here to perform it. There are other, simpler, tests (like Kolmogorov-Smirnov), but I like Shapiro-Wilk's because it's meant specifically for normal distributions. \\

First, we'll verify that the test works by testing it against a normal distribution and a non-normal distribution.

#+begin_src R :results output :exports both :tangle yes :session
(cbind(shapiro.test(rnorm(draws))[1:2], shapiro.test(rpois(draws,10))[1:2]))
#+end_src

#+RESULTS:
:           [,1]      [,2]
: statistic 0.9975899 0.9871238
: p.value   0.1498872 1.070178e-07

Great! The null is that the distribution is distributed normal, so we fail to reject (at any reasonable level) the null for the normal distribution and reject the null with a great deal of confidence for the poisson distribution. Now let's try it on our estimated coefficients:

#+begin_src R :results output :exports both :tangle yes :session
(cbind(
  shapiro.test(blist.0[ ,2])[1:2],
  shapiro.test(blist.1[ ,2])[1:2],
  shapiro.test(blist.2[ ,2])[1:2],
  shapiro.test(blist.3[ ,2])[1:2]))
#+end_src

#+RESULTS:
:           [,1]      [,2]      [,3]      [,4]
: statistic 0.9974901 0.9983627 0.9981639 0.9988518
: p.value   0.127683  0.4682959 0.3581798 0.7879152

Great! We fail to reject the null for all three distributions, so we can feel some confidence that our coefficients are truly distributed normal! All is well in asymptopia!
